{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from pprint import pprint\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для получения данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, params = '', headers ='', proxy = ''): \n",
    "        r = requests.get(url, params, headers=headers, proxies=proxy)\n",
    "        return r.text\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для выбора случайного юзер-агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_user_agent(user_agents):\n",
    "        user_agent = user_agents[random.randrange(len(user_agents))]\n",
    "        headers = {'user-agent': user_agent}\n",
    "        return headers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение данных юзер-агентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Lenovo\\Desktop\\diplom\\desktop_user_agent.txt', 'r', encoding='utf-8') as fp:\n",
    "    user_agents = [line.strip() for line in fp.read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = f'https://career.habr.com/vacancies?page={page_number}&q=аналитик&type=all'\n",
    "#data = get_data(url = url, headers = choose_user_agent(user_agents))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id собранных ранее вакансий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'C:\\Users\\Lenovo\\Desktop\\diplom\\habr_data.json', 'r', encoding='utf-8') as fp:\n",
    "        all_vacs = json.load(fp)\n",
    "all_ids = set()\n",
    "for vac in all_vacs:\n",
    "    all_ids.add(vac['id'])\n",
    "len(all_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение данных о вакансиях и отбор только тех, что нет в общем файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 finished\n",
      "2 finished\n",
      "3 finished\n",
      "4 finished\n",
      "5 finished\n",
      "6 finished\n",
      "7 finished\n",
      "8 finished\n",
      "9 finished\n",
      "10 finished\n",
      "11 finished\n",
      "12 finished\n",
      "13 finished\n",
      "14 finished\n",
      "15 finished\n",
      "16 finished\n",
      "17 finished\n",
      "18 finished\n",
      "19 finished\n",
      "20 finished\n",
      "21 finished\n",
      "22 finished\n",
      "23 finished\n",
      "24 finished\n"
     ]
    }
   ],
   "source": [
    "vac_info = []\n",
    "page_number = 1\n",
    "while True:\n",
    "    #print(f'{page_number} started')\n",
    "    url = f'https://career.habr.com/vacancies?locations[]=ct_444&page={page_number}&q=аналитик&type=all'\n",
    "    data = get_data(url = url, headers = choose_user_agent(user_agents))\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    blocks = soup.find_all('a', attrs = {'class': 'vacancy-card__title-link'})\n",
    "    if len(blocks) == 0:\n",
    "        break\n",
    "    for block in blocks:\n",
    "        vac_id = block['href']\n",
    "        if int(vac_id[11:]) in all_ids:\n",
    "            continue\n",
    "        if re.search('аналитик|BI|analyst', block.text.lower()) == None:\n",
    "            continue\n",
    "        #vac_id = '/vacancies/1000115845'\n",
    "        vac_url = f'https://career.habr.com{vac_id}'\n",
    "        vac_data = get_data(vac_url, headers = choose_user_agent(user_agents))\n",
    "        area = ''\n",
    "        description = ''\n",
    "        employment = ''\n",
    "        experience = ''\n",
    "        id = ''\n",
    "        key_skills = [] \n",
    "        name = ''\n",
    "        publication_date = ''\n",
    "        salary = ''\n",
    "        schedule = ''\n",
    "        specializations = ''\n",
    "        proffesional_roles= ''\n",
    "        employer = ''\n",
    "        vac_soup = BeautifulSoup(vac_data, 'lxml').find('div', attrs = {'class': 'vacancy-description__text'})\n",
    "        try: \n",
    "            description = vac_soup.get_text(' ')\n",
    "        except:\n",
    "            continue\n",
    "        vac_soup = BeautifulSoup(vac_data, 'lxml').find_all('script', attrs = {'type': 'application/json'})[0].text\n",
    "        vac_json = json.loads(vac_soup)\n",
    "        name = vac_json['vacancy']['title']\n",
    "        id = vac_json['vacancy']['id']\n",
    "        salary = vac_json['vacancy']['salary']\n",
    "        for skill in vac_json['vacancy']['skills']:\n",
    "            key_skills.append(skill['title'])\n",
    "        try: \n",
    "            experience = vac_json['vacancy']['salaryQualification']['title']\n",
    "        except:\n",
    "            experience = ''\n",
    "        employment = vac_json['vacancy']['employmentType']\n",
    "        employer = vac_json['vacancy']['company']['title']\n",
    "        publication_date = vac_json['vacancy']['publishedDate']['date']\n",
    "        area = vac_json['vacancy']['shortGeo']\n",
    "        if vac_json['vacancy']['remoteWork'] == 'True':\n",
    "            schedule = 'Можно удаленно'\n",
    "        vac_info.append({\n",
    "            'area': area,\n",
    "            'description': description,\n",
    "            'employment': employment,\n",
    "            'experience': experience,\n",
    "            'id': id,\n",
    "            'key_skills': key_skills,\n",
    "            'name': name,\n",
    "            'publication_date': publication_date,\n",
    "            'salary': salary,\n",
    "            'schedule': schedule,\n",
    "            'specializations': specializations,\n",
    "            'proffesional_roles': proffesional_roles,\n",
    "            'employer': employer,\n",
    "            'source': 'career.habr.com'}\n",
    "        )\n",
    "    print(f'{page_number} finished')\n",
    "    page_number += 1\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для записи полученных за день данных в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_to_file(vac_info):\n",
    "    with open(r'C:\\Users\\Lenovo\\Desktop\\diplom\\habr_data.json', 'r', encoding='utf-8') as fp:\n",
    "        all_vacs = json.load(fp)\n",
    "    all_vacs = all_vacs + vac_info\n",
    "    with open(r'C:\\Users\\Lenovo\\Desktop\\diplom\\habr_data.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(all_vacs, fp, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранения данных в excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_json_to_excel():\n",
    "    with open(r'C:\\Users\\Lenovo\\Desktop\\diplom\\habr_data.json', 'r', encoding='utf-8') as fp:\n",
    "        to_excel = json.load(fp)\n",
    "    df = pd.DataFrame(to_excel)\n",
    "    df.to_excel(r'C:\\Users\\Lenovo\\Desktop\\diplom\\habr_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data_to_file(vac_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from_json_to_excel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f8cf99212f81ad0c4765861267244807ee1c18364846cdce387b28994a977ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
