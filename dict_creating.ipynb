{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import tools as t\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение данных о скилах во всех вакансиях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.read_json('deduplicated.json')['key_skills']\n",
    "skills = []\n",
    "for element in full:\n",
    "    if len(element) > 0:\n",
    "        skills = skills + element\n",
    "print(len(skills))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Навыки, которые не нужно разделять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_to_split = ['a/b', 'ci/cd', 'pl/sql', 'pl/pg', 'tcp/ip', 'ui/ux', 'ux/ui', 'а/б', 'а/в', 'в/из', 'c++']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделение перечислений на отдельные навыки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = t.split_skills(skills, not_to_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.DataFrame(skills)\n",
    "dff['orig'] = dff[0]\n",
    "dff = dff.drop(columns = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление нерелевантных символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['clean'] = dff['orig'].apply(lambda x: t.clean_text(x, not_to_split))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['tokenize'] = dff['clean'].apply(lambda x: word_tokenize(x, language='russian'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['lemmatize'] = dff['tokenize'].apply(lambda x: t.lemmatization(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачивание стоп-слов для русского языка и добавление новых стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_to_add = ['хорошее', 'знание', 'знать', 'опыт', 'работы', 'организация', 'работать', 'ms', 'условия', 'умение', 'навык']\n",
    "sw_to_add = t.lemmatization(sw_to_add)\n",
    "stopwords = stopwords.words('russian')\n",
    "stopwords = stopwords + sw_to_add"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['lemmatize'] = dff['lemmatize'].apply(lambda x: t.remove_stop_words(x, stopwords))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составление n-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['bigrams'] = dff['lemmatize'].apply(lambda x: list(ngrams(x, 2)))\n",
    "dff['trigrams'] = dff['lemmatize'].apply(lambda x: list(ngrams(x, 3)))\n",
    "dff['fourgrams'] = dff['lemmatize'].apply(lambda x: list(ngrams(x, 4)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение частоты n-грамм, для которых частота >=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = t.most_frequent_ngrams(dff['lemmatize'], 1)\n",
    "most_common_bigrams = t.most_frequent_ngrams(dff['bigrams'], 1)\n",
    "most_common_trigrams = t.most_frequent_ngrams(dff['trigrams'], 1)\n",
    "most_common_fourgrams = t.most_frequent_ngrams(dff['fourgrams'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(most_common[0:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение полученных n-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams = pd.concat([pd.DataFrame({'unigrams': most_common}), pd.DataFrame({'bigrams': most_common_bigrams}), pd.DataFrame({'trigrams': most_common_trigrams}), pd.DataFrame({'fourgrams': most_common_fourgrams})], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_skill_amount(most_common):\n",
    "    ngram = []\n",
    "    ngram_amount = []\n",
    "    for one in most_common:\n",
    "        ngram.append(str(one[0]))\n",
    "        ngram_amount.append(one[1])\n",
    "    return ngram, ngram_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni, uni_amount = split_skill_amount(most_common)\n",
    "bi, bi_amount = split_skill_amount(most_common_bigrams)\n",
    "tri, tri_amount = split_skill_amount(most_common_trigrams)\n",
    "four, four_amount = split_skill_amount(most_common_fourgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = pd.DataFrame({'name': uni, 'amount': uni_amount})\n",
    "bigrams = pd.DataFrame({'name': bi, 'amount': bi_amount})\n",
    "trigrams = pd.DataFrame({'name': tri, 'amount': tri_amount})\n",
    "fourgrams = pd.DataFrame({'name': four, 'amount': four_amount})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "считать количество во всех n+1 граммах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one in trigrams['name']:\n",
    "    if fourgrams['name'].str.contains(one).any() == True:\n",
    "        ind = trigrams.index[trigrams['name'] == one].tolist()[0]\n",
    "        trigrams['amount'][ind] = trigrams['amount'][ind] - fourgrams[fourgrams['name'].str.contains(one)]['amount'].sum()\n",
    "        if trigrams['amount'][ind] < 0:\n",
    "            trigrams = trigrams[trigrams.name != one]\n",
    "\n",
    "for one in bigrams['name']:\n",
    "    if trigrams['name'].str.contains(one).any() == True:\n",
    "        ind = bigrams.index[bigrams['name'] == one].tolist()[0]\n",
    "        bigrams['amount'][ind] = bigrams['amount'][ind] - (trigrams[trigrams['name'].str.contains(one)]['amount'].sum() + \n",
    "                                                           fourgrams[fourgrams['name'].str.contains(one)]['amount'].sum())\n",
    "        if bigrams['amount'][ind] < 0:\n",
    "            bigrams = bigrams[bigrams.name != one]\n",
    "\n",
    "for index, one in enumerate(unigrams['name']):\n",
    "    search = one\n",
    "    try:\n",
    "        bigrams['name'].str.contains(\"'\" + one + \"'\").any()\n",
    "    except:\n",
    "        print(one, index)\n",
    "    if bigrams['name'].str.contains(\"'\" + one + \"'\").any() == True:\n",
    "        ind = unigrams.index[unigrams['name'] == search].tolist()[0]\n",
    "        unigrams['amount'][ind] = unigrams['amount'][ind] - (bigrams[bigrams['name'].str.contains(one)]['amount'].sum() + \n",
    "                                                             trigrams[trigrams['name'].str.contains(one)]['amount'].sum() + \n",
    "                                                             fourgrams[fourgrams['name'].str.contains(one)]['amount'].sum())\n",
    "        if unigrams['amount'][ind] < 0:\n",
    "            unigrams = unigrams[unigrams.name != search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unigrams), len(bigrams), len(trigrams), len(fourgrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_count = unigrams.amount.sum() + bigrams.amount.sum() + trigrams.amount.sum() + fourgrams.amount.sum()\n",
    "print(skills_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление \"не навыков\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete_from_dict = pd.read_csv('words_to_delete_from_dict.csv')['0'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full = pd.concat([unigrams, bigrams, trigrams, fourgrams], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full['name'] = ngrams_full['name'].apply(lambda x: None if x in to_delete_from_dict else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full.sort_values(by = 'amount', ascending = False, inplace = True, ignore_index = True)\n",
    "ngrams_full['cumperc'] = ngrams_full['amount'].cumsum()/ngrams_full['amount'].sum()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full = ngrams_full[ngrams_full['cumperc'] < 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full.to_excel('dict-pareto_v9.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "удаление ',()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full['name'] = ngrams_full['name'].apply(lambda x: re.sub(\"[\\'(),]\", '', x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full['name'].to_excel('automatic generated dict.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f8cf99212f81ad0c4765861267244807ee1c18364846cdce387b28994a977ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
